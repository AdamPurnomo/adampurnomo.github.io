<!doctype html>
<html lang="en">



<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Mark Otto, Jacob Thornton, and Bootstrap contributors">
    <meta name="generator" content="Hugo 0.88.1">
    <title>Adam Purnomo</title>

    <link rel="canonical" href="https://getbootstrap.com/docs/5.1/examples/album/">
    <link rel="icon" href="../media/head-icon.gif">



    <!-- Bootstrap core CSS -->
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link href="../stylesheet/style.css" rel="stylesheet">

    <!-- Favicons -->
    <link rel="apple-touch-icon" href="/docs/5.1/assets/img/favicons/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" href="/docs/5.1/assets/img/favicons/favicon-32x32.png" sizes="32x32" type="image/png">
    <link rel="icon" href="/docs/5.1/assets/img/favicons/favicon-16x16.png" sizes="16x16" type="image/png">
    <link rel="manifest" href="/docs/5.1/assets/img/favicons/manifest.json">
    <link rel="mask-icon" href="/docs/5.1/assets/img/favicons/safari-pinned-tab.svg" color="#7952b3">
    <link rel="icon" href="/docs/5.1/assets/img/favicons/favicon.ico">
    <meta name="theme-color" content="#7952b3">


    <style>
        .bd-placeholder-img {
            font-size: 1.125rem;
            text-anchor: middle;
            -webkit-user-select: none;
            -moz-user-select: none;
            user-select: none;
        }

        @media (min-width: 768px) {
            .bd-placeholder-img-lg {
                font-size: 3.5rem;
            }
        }
    </style>


</head>

<body style="background-color: #ffffff;
background-image: linear-gradient(315deg, #ffffff 50%, #d7e1ec 74%);">
    <header>
        <nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark bg-gradient">
            <div class=" container-fluid">

                <a class="navbar-brand" href="../index.html"> <img src="../media/images/ap-icon.gif" height="30px"
                        width="30px" />
                    &nbsp; &nbsp;Adam Purnomo</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false"
                    aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
                    <div class="navbar-nav ms-auto">
                        <a class="nav-link ms-auto" href="../index.html">Home</a>
                        <a class="nav-link ms-auto" href="../media/resume.pdf" target="_blank"
                            rel="noopener noreferrer">Curriculum
                            Vitae</a>
                        <a class="nav-link ms-auto" href="../rpojects.html">Research Projects</a>
                        <a class="nav-link ms-auto" href="../sprojects.html">Personal Projects</a>
                        <a class="nav-link ms-auto" href="../blogs.html">Blog</a>
                    </div>
                </div>
            </div>
        </nav>

    </header>

    <!-- this is the body -->
    <main>
        <div class="container">
            <div class="pt-5 text-center">
                <h2>Deep Learning-based 6-DOF Grasp Estimation for Industrial Bin-Picking</h2>
                <div class="bar"></div>
            </div>

            <div class="pt-4 pb-2">
                <h4>Overview</h4>
            </div>

            <div>
                <p class="text-justify">
                    This project focus on the problem of the 6-DoF parallel-jaw grasp for bin-picking in an industrial
                    setting. We explore a method to estimate 6-DoF grasp from bin-picking scenes in an industrial
                    setting using a convolutional neural network (CNN) without having to estimate the 6D pose of the
                    target object. We gained inspiration from the idea that the robot does not need to know exactly the
                    precise 6D pose of the target object to get a reasonably good grasp. A rough estimate on how to
                    approach the object is often more than sufficient to grasp the target object. We use what we call
                    grasp approaching pose vector which determines from which direction the robot gripper should
                    approach the target object in Cartesian space. The network evaluates the grasp candidates
                    represented as grasp rectangles taken from a single depth image and outputs the 2D projection of the
                    grasp approaching pose vector at once. This 2D projection can later be converted back to a 3D vector
                    with the knowledge of the camera intrinsic matrix. For more detail, please take look at
                    <a href="/media/Deep Learning based 6 DoF Grasp Estimation for Industrial Bin Picking.pdf"
                        target="_blank" rel="noopener noreferrer">Deep
                        Learning-based 6-DoF Grasp Estimation for Industrial Bin-Picking
                    </a> (manuscript in preparation).
                </p>
                <figure>
                    <img class="detail-img" src="/media/images/research/grasp planning/overview.png" style="    width: 100%;
                    height: 30vw;" />
                    <figcaption class="text-center caption">Overview of the proposed method.</figcaption>
                </figure>

                <figure>
                    <iframe width="450" height="300" src="https://youtube.com/embed/rVky5cjJaMA" frameborder="0"
                        allowfullscreen></iframe>
                    <figcaption class="text-center caption">Short Explanation Video (Presented at SICE-SI 2020)
                    </figcaption>
                </figure>

            </div>

            <div class="pt-4 pb-2">
                <h4>Network Architecture</h4>
            </div>

            <div>
                <p class="text-justify">
                    The proposed network architecture is shown in the figure below. The network takes inputs of
                    individual grasp
                    image and outputs the grasping score and the 2D projection of
                    the grasp approaching pose vector. The network is divided into
                    three parts: feature extractor, grasp pose estimator and grasp
                    quality estimator. The feature extractor consists of 4 stages
                    of convolution processes where each stage is comprised of 2
                    units of residual blocks. The grasp pose estimator consists
                    of 3 stages of deconvolution processes, and each stage is also
                    comprised of 2 units of residual blocks
                </p>

                <figure>
                    <img class="detail-img" src="/media/images/research/grasp planning/arch.png" style="    width: 100%;
                    height: 35vw;" />
                    <figcaption class="text-center caption">Overview of the proposed method.</figcaption>
                </figure>

                <p class="text-justify">
                    Inspired from the architecture of U-Net, the output of
                    the first three stages of convolution processes in the feature
                    extractor part are forwarded and concatenated to the input of
                    corresponding deconvolution processes with the same size as shown in the figure above. The last part
                    is the grasp quality estimator
                    which consists of 2 dense layers with 512 and 256 nodes each.
                    Instead of using max-pooling with fixed sliding window size,
                    we use global max-pooling for the input of the first dense layer
                    in the grasp quality estimator to make sure that the network
                    can take an arbitrary size of input images
                </p>
            </div>

            <div class="pt-4 pb-2">
                <h4>Experiment with A Real Robot</h4>
            </div>
            <div>
                <p class="text-justify">
                    We evaluate the proposed method by conducting real bin-picking experiments. The experimental setup
                    of the experiment is shown in the figure below. We used a 6-DoF
                    Denso robotic arm as the manipulator, a pneumatic parallel
                    gripper, and an Ensenso N-30 depth camera with 1280 x 1024
                    resolution for the 3D sensor.
                </p>
                <figure>
                    <img class="detail-img" src="/media/images/research/grasp planning/experimental setup.png" style="    width: 100%;
                    height: 30vw;" />
                    <figcaption class="text-center caption">Experimental Setup.</figcaption>
                </figure>
                <p>
                    We validated our method with 4 types of industrial objects. We found that the proposed method
                    performed reasonably well in all experiments with a grasp success rate ranging from 84.21% to
                    89.62%. The experiment video can be seen below.
                </p>

                <figure>
                    <iframe width="450" height="300" src="https://youtube.com/embed/J0o0fcqUbLQ" frameborder="0"
                        allowfullscreen></iframe>
                    <figcaption class="text-center caption">Experiment results with 4 different types of objects.
                    </figcaption>
                </figure>
            </div>

            <div class="pt-3 text-center">
                <h5>Source Code : </h5> <a href="https://github.com/AdamPurnomo/Deep-Learning-based-Grasp-Planning"
                    target="_blank" rel="noopener noreferrer"><img src="../media/images/github.png" height="40px"
                        width="40px"></a>
            </div>
        </div>
    </main>

    <!-- this is the body -->

    <!-- JavaScript Bundle with Popper -->
    <script src=" https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous">
        </script>

    <footer class="py-5">
        <div class="container text-center">
            <p>© Adam Purnomo </p>
        </div>
    </footer>
</body>

</html>